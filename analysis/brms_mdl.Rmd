---
title: "brms_mdl"
output: html_document
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_width: 6
    fig_height: 4
    fig_caption: true
    theme: default
---

```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse,     # wrangling
  brms,          # Bayesian modeling
  data.table,    # optional
  janitor,       # optional
  here,          # reproducible paths
  bayesplot,     # MCMC diagnostics
  posterior,     # posterior handling
  loo            # model comparison
)

#install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#cmdstanr::install_cmdstan()
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())
theme_set(theme_minimal())

seed <- 521
```

## Load and prepare data

We load preprocessed data from the pilot and normalize perceived and Euclidean distances.


```{r load-data}
df <- read_csv("cleandata/pilot_internal_preprocessed.csv")
df <- na.omit(df)  # remove row with missing data (we should actually not have these.)

df <- df %>%
  mutate(
    euclidean_norm = euclideanDistance / max(euclideanDistance, na.rm = TRUE),
    perceived_norm = perceivedDistance / max(perceivedDistance, na.rm = TRUE)
  )
```

## Baseline model 1: random intercept only

This model assumes a fixed population slope between perceived and actual distance, with varying intercepts by participant.

```{r, include=FALSE}
m1 <- brm(
  formula = perceived_norm ~ euclidean_norm + (1 | code),
  data = df,
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "Intercept"),
    prior(normal(1, 1), class = "b"),
    prior(exponential(1), class = "sd")
  ),
  chains = 4, iter = 4000, warmup = 2000, seed = seed,
  control = list(adapt_delta = 0.99, max_treedepth = 20)
)

```

### pp check

Some funky structure in our data with 2 peaks--wonder whether this is friends vs. other types. 

```{r}
pp_check(m1)
```

### fixed effects
Looks reasonable;
1. essentially 1-to-1 correspondence between euclidean norm (objective distance) and perceived distance.

```{r}
fixef(m1)

```



## Basline model 2: random intercepts and slopes

```{r, include=FALSE}

m2 <- brm(
    formula = perceived_norm ~ euclidean_norm + (1 + euclidean_norm | code),
    data = df,
    family = gaussian(),
    prior = c(
        prior(normal(0, 1), class = "Intercept"),
        prior(normal(1, 1), class = "b"),
        prior(exponential(1), class = "sd"),
        prior(lkj(2), class = "cor")
    ),
    chains = 4, iter = 4000, warmup = 2000, cores = 4, seed = seed,
    control = list(adapt_delta = 0.99, max_treedepth = 20)
)

```

### pp check (similar to above)

```{r}
pp_check(m2)
```

### fixed effects

Very similar to model 1.

```{r}
fixef(m2)
```


## Model comparison (out-of-sample cross-validation)

```{r}

loo_1 <- loo(m1)
loo_2 <- loo(m2) # best model
loo_compare(loo_1, loo_2)

```

## Visualize Models 1 and 2

Individual predictions

```{r}

# Add fitted values to the data
df_fitted <- df %>%
    mutate(
        fitted_model1 = fitted(m1)[, "Estimate"],
        fitted_model2 = fitted(m2)[, "Estimate"]
    )

# Sample participants 
sampled_codes <- sample(unique(df$code), 9)

# Plot 
df_fitted %>%
    filter(code %in% sampled_codes) %>%
    pivot_longer(cols = starts_with("fitted_model"), names_to = "model", values_to = "fitted") %>%
    ggplot(aes(x = euclidean_norm, y = perceived_norm)) +
    geom_point(alpha = 0.6, size = 2) +
    geom_line(aes(y = fitted, color = model)) +
    facet_wrap(~code, nrow = 3) +
    scale_color_manual(values = c("fitted_model1" = "gray40", "fitted_model2" = "steelblue")) +
    labs(
        title = "Observed vs Fitted Perceived Distances",
        subtitle = "Random intercept vs intercept + slope models",
        x = "Euclidean Distance",
        y = "Perceived Distance",
        color = "Model"
    ) +
    theme_minimal(base_size = 14) +
    theme(
        legend.position = "bottom",
        strip.text = element_text(size = 12),
        plot.title = element_text(size = 16, face = "bold"),
        plot.subtitle = element_text(size = 12)
    )

```

## Visualize Models 1 and 2

Population effect.

```{r}
# A grid of X values from 0 to 1 (normalized scale)
new_data <- tibble(
    euclidean_norm = seq(0, 1, length.out = 100),
    code = NA # force population-level prediction (not per participant)
)
# Get population-level predictions (marginal over random effects)
fit1 <- fitted(m1, newdata = new_data, re_formula = NA) %>% as_tibble()
fit2 <- fitted(m2, newdata = new_data, re_formula = NA) %>% as_tibble()

# Combine with prediction grid and tag model
pop_preds <- bind_rows(
    bind_cols(new_data, fit1) %>% mutate(model = "m1"),
    bind_cols(new_data, fit2) %>% mutate(model = "2")
)

ggplot(pop_preds, aes(x = euclidean_norm, y = Estimate, color = model, fill = model)) +
    geom_line() +
    geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.2, color = NA) +
    scale_color_manual(values = c("m1" = "gray40", "m2" = "steelblue")) +
    scale_fill_manual(values = c("m1" = "gray40", "m2" = "steelblue")) +
    labs(
        title = "Population-Level Prediction",
        subtitle = "Mean and 95% credible interval",
        x = "Euclidean Distance (normalized)",
        y = "Perceived Distance (normalized)",
        color = "Model",
        fill = "Model"
    ) +
    theme_minimal(base_size = 14) +
    theme(
        legend.position = "bottom",
        plot.title = element_text(size = 16, face = "bold"),
        plot.subtitle = element_text(size = 12)
    ) +
    geom_point(data = df, aes(x = euclidean_norm, y = perceived_norm), inherit.aes = FALSE, alpha = 0.1)

```

Does not look so different, but apparently slight preference for model 2 (including random slopes).

## Model 3: ...


```{r, include=FALSE}

m3 <- brm(
    formula = perceived_norm ~ d_climate_concern + d_gay_adoption + d_migration_enriches_culture + d_govt_reduce_inequ + (1 | code),
    data = df,
    family = gaussian(),
    prior = c(
        prior(normal(0, 1), class = "Intercept"), # between (-2, +2)
        prior(normal(0, 1), class = "b"), # here lower than before (center at 0 for now)
        prior(exponential(1), class = "sd")
    ),
    chains = 4, iter = 4000, warmup = 2000, cores = 4, seed = seed,
    control = list(adapt_delta = 0.99, max_treedepth = 20)
)


```

```{r}
pp_check(m3)
```

### fixed effects

This looks more weird, we have
1. Much higher intercept (when all of d_* are 0), which should still be low (euclidean distance also 0)
2. All of the betas should be positive (logically)--how is govt_reduce_inequ negative? 

Must be something here that does not quite work. 

```{r}
fixef(m3) 
# mcmc_areas(as_draws_df(m3), pars = vars(starts_with("b_")))
```

## Model comparison loo()

Yes, this model is incredibly worse.

```{r}

loo_3 <- loo(m3)
loo_compare(loo_1, loo_2, loo_3) # model 3 is terrible.

```

## Model 4: 

```{r}

```

